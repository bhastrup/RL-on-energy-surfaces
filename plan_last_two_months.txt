

January 22-24:
Finalize model and write internal coordinate section (since this is unlikely to change much).

25-28:
Writing: describe environment (unlikely to change much from now on)
Coding: action space output (probably a necessary model extension)
29-31:
Describe agent
Coding: Bootstrap (Q-learning)


Chapter - Introduction
Transitions path methods
DFT, NEB shortcomings
Litterature review - molgym, schnet ...

Chapter - Theory
Basic RL theory
Deep RL
Monte carlo control
Q-learning
Policy gradient and Actor Critic
Neural networks
- Graph neural networks

Chapter - Environment:
What is a state?
Action space
Step transition physics
Reward shaping


Chapter - Agent model:
Decision model, graph nn

Chapter - Experiments:
a) Simple map, hollow-to-hollow
- compare with random agent
b) Agent atom swap


Chapter conclusion
